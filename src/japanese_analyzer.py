#!/usr/bin/env python3
"""
æ—¥æœ¬èªå½¢æ…‹ç´ è§£æå™¨ - Phase 2å®Ÿè£…
MeCabä»£æ›¿ã¨ã—ã¦è»½é‡ãªæ—¥æœ¬èªè§£ææ©Ÿèƒ½ã‚’æä¾›
"""

import re
from typing import List, Dict, Tuple, Optional
import json
from pathlib import Path

class JapaneseAnalyzer:
    """
    æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®å½¢æ…‹ç´ è§£æãƒ»åˆ†æã‚’è¡Œã†ã‚¯ãƒ©ã‚¹
    MeCabã®ä»£æ›¿ã¨ã—ã¦åŸºæœ¬çš„ãªæ—¥æœ¬èªå‡¦ç†æ©Ÿèƒ½ã‚’æä¾›
    """
    
    def __init__(self, custom_dict_path: Optional[str] = None):
        """
        æ—¥æœ¬èªè§£æå™¨ã®åˆæœŸåŒ–
        
        Args:
            custom_dict_path: ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.hiragana_pattern = re.compile(r'[ã²-ã‚]+')
        self.katakana_pattern = re.compile(r'[ã‚¡-ãƒ¾]+')
        self.kanji_pattern = re.compile(r'[ä¸€-é¾¯]+')
        self.ascii_pattern = re.compile(r'[a-zA-Z0-9]+')
        
        # åŸºæœ¬çš„ãªåŠ©è©ãƒ»åŠ©å‹•è©ãƒ»æ¥ç¶šè©
        self.particles = {
            'ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã§', 'ã¨', 'ã®', 'ã‚„', 'ã‹', 'ã‚‚', 'ã‹ã‚‰', 'ã¾ã§',
            'ã‚ˆã‚Š', 'ã¸', 'ã°', 'ã¦', 'ã§', 'ãŸ', 'ã ', 'ã§ã‚ã‚‹', 'ã§ã™', 'ã¾ã™',
            'ã—ãŸ', 'ã—ã¾ã™', 'ã•ã‚Œã‚‹', 'ã™ã‚‹', 'ã§ãã‚‹', 'ãªã‚‹', 'ã„ã‚‹', 'ã‚ã‚‹'
        }
        
        # Ultra Payé–¢é€£ã®å°‚é–€ç”¨èªè¾æ›¸
        self.technical_terms = {
            # ãƒ—ãƒªãƒšã‚¤ãƒ‰ã‚«ãƒ¼ãƒ‰é–¢é€£
            'ãƒ—ãƒªãƒšã‚¤ãƒ‰ã‚«ãƒ¼ãƒ‰', 'ãƒ—ãƒªãƒšã‚¤ãƒ‰', 'UltraPay', 'PayBlend', 'VISA',
            'ã‚»ãƒ–ãƒ³éŠ€è¡Œ', 'ATM', 'QRã‚³ãƒ¼ãƒ‰', 'ãƒãƒ£ãƒ¼ã‚¸', 'æ®‹é«˜', 'æ±ºæ¸ˆ',
            
            # æŠ€è¡“ç”¨èª
            'API', 'JWT', 'OAuth', 'SSL', 'TLS', 'JSON', 'XML', 'HTTP', 'HTTPS',
            'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹', 'ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹', 'ãƒˆãƒ¼ã‚¯ãƒ³', 'ãƒ­ã‚°ã‚¤ãƒ³', 'èªè¨¼', 'ã‚»ãƒƒã‚·ãƒ§ãƒ³',
            
            # é–‹ç™ºç”¨èª
            'Laravel', 'Vue.js', 'Docker', 'Kubernetes', 'AWS', 'S3', 'Lambda',
            'ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰', 'ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰', 'ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹', 'CI/CD',
            
            # æ¥­å‹™ç”¨èª
            'ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ', 'ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰', 'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£', 'ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³',
            'ã‚¨ãƒ©ãƒ¼', 'ãƒ­ã‚°', 'ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹', 'ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—', 'ãƒªã‚¹ãƒˆã‚¢'
        }
        
        # ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ã®èª­ã¿è¾¼ã¿
        if custom_dict_path and Path(custom_dict_path).exists():
            self.load_custom_dictionary(custom_dict_path)
    
    def load_custom_dictionary(self, dict_path: str):
        """
        ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            dict_path: è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        try:
            with open(dict_path, 'r', encoding='utf-8') as f:
                custom_dict = json.load(f)
                
            # compound_termsã‹ã‚‰å°‚é–€ç”¨èªã‚’è¿½åŠ 
            if 'compound_terms' in custom_dict:
                for term, data in custom_dict['compound_terms'].items():
                    self.technical_terms.add(term)
                    # åŒç¾©èªã‚‚è¿½åŠ 
                    if 'synonyms' in data:
                        for synonym in data['synonyms']:
                            if self._is_japanese(synonym):
                                self.technical_terms.add(synonym)
                                
        except Exception as e:
            print(f"ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
    
    def analyze(self, text: str) -> List[Dict[str, str]]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’å½¢æ…‹ç´ è§£æã™ã‚‹
        
        Args:
            text: è§£æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            å½¢æ…‹ç´ æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        morphemes = []
        tokens = self.tokenize(text)
        
        for token in tokens:
            if not token.strip():
                continue
                
            morpheme = {
                'surface': token,
                'pos': self._get_part_of_speech(token),
                'base_form': self._get_base_form(token),
                'reading': self._get_reading(token)
            }
            morphemes.append(morpheme)
            
        return morphemes
    
    def tokenize(self, text: str) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†å‰²ã™ã‚‹
        
        Args:
            text: åˆ†å‰²å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ
        """
        tokens = []
        i = 0
        
        while i < len(text):
            # å°‚é–€ç”¨èªã®æœ€é•·ä¸€è‡´
            longest_match = self._find_longest_technical_term(text, i)
            if longest_match:
                tokens.append(longest_match)
                i += len(longest_match)
                continue
            
            # è‹±æ•°å­—ã®å‡¦ç†
            if text[i].isascii() and text[i].isalnum():
                match = self.ascii_pattern.match(text, i)
                if match:
                    tokens.append(match.group())
                    i = match.end()
                    continue
            
            # æ¼¢å­—ã®å‡¦ç†
            if self.kanji_pattern.match(text[i]):
                kanji_token = self._extract_kanji_compound(text, i)
                tokens.append(kanji_token)
                i += len(kanji_token)
                continue
            
            # ã‚«ã‚¿ã‚«ãƒŠã®å‡¦ç†
            if self.katakana_pattern.match(text[i]):
                katakana_match = self.katakana_pattern.match(text, i)
                if katakana_match:
                    tokens.append(katakana_match.group())
                    i = katakana_match.end()
                    continue
            
            # ã²ã‚‰ãŒãªã®å‡¦ç†
            if self.hiragana_pattern.match(text[i]):
                hiragana_token = self._extract_hiragana_token(text, i)
                tokens.append(hiragana_token)
                i += len(hiragana_token)
                continue
            
            # ãã®ä»–ã®æ–‡å­—ï¼ˆè¨˜å·ç­‰ï¼‰
            if text[i] not in ' \t\n\r':
                tokens.append(text[i])
            i += 1
        
        return tokens
    
    def extract_compound_words(self, text: str) -> List[str]:
        """
        è¤‡åˆèªã‚’æŠ½å‡ºã™ã‚‹
        
        Args:
            text: æŠ½å‡ºå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            è¤‡åˆèªã®ãƒªã‚¹ãƒˆ
        """
        compounds = []
        tokens = self.tokenize(text)
        
        # é€£ç¶šã™ã‚‹åè©ãƒ»å½¢å®¹è©ã‚’è¤‡åˆèªã¨ã—ã¦æŠ½å‡º
        i = 0
        while i < len(tokens):
            if self._is_noun_like(tokens[i]):
                compound = [tokens[i]]
                j = i + 1
                
                while j < len(tokens) and self._is_noun_like(tokens[j]):
                    compound.append(tokens[j])
                    j += 1
                
                if len(compound) >= 2:
                    compounds.append(''.join(compound))
                i = j
            else:
                i += 1
        
        return compounds
    
    def _find_longest_technical_term(self, text: str, start: int) -> Optional[str]:
        """
        æŒ‡å®šä½ç½®ã‹ã‚‰å§‹ã¾ã‚‹æœ€é•·ã®å°‚é–€ç”¨èªã‚’æ¤œç´¢
        
        Args:
            text: æ¤œç´¢å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            start: æ¤œç´¢é–‹å§‹ä½ç½®
            
        Returns:
            è¦‹ã¤ã‹ã£ãŸå°‚é–€ç”¨èªï¼ˆãªã‘ã‚Œã°Noneï¼‰
        """
        longest_match = None
        max_length = 0
        
        for term in self.technical_terms:
            if text[start:start + len(term)] == term:
                if len(term) > max_length:
                    longest_match = term
                    max_length = len(term)
        
        return longest_match
    
    def _extract_kanji_compound(self, text: str, start: int) -> str:
        """
        æ¼¢å­—è¤‡åˆèªã‚’æŠ½å‡º
        
        Args:
            text: å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            start: é–‹å§‹ä½ç½®
            
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸæ¼¢å­—è¤‡åˆèª
        """
        end = start
        while end < len(text) and self.kanji_pattern.match(text[end]):
            end += 1
        
        return text[start:end] if end > start else text[start]
    
    def _extract_hiragana_token(self, text: str, start: int) -> str:
        """
        ã²ã‚‰ãŒãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡º
        
        Args:
            text: å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ  
            start: é–‹å§‹ä½ç½®
            
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸã²ã‚‰ãŒãªãƒˆãƒ¼ã‚¯ãƒ³
        """
        end = start
        
        # åŠ©è©ãƒ»åŠ©å‹•è©ã®åˆ¤å®š
        for particle in sorted(self.particles, key=len, reverse=True):
            if text[start:start + len(particle)] == particle:
                return particle
        
        # ä¸€èˆ¬çš„ãªã²ã‚‰ãŒãªé€£ç¶š
        while end < len(text) and self.hiragana_pattern.match(text[end]):
            end += 1
            
        return text[start:end] if end > start else text[start]
    
    def _get_part_of_speech(self, token: str) -> str:
        """
        å“è©ã‚’æ¨å®š
        
        Args:
            token: ãƒˆãƒ¼ã‚¯ãƒ³
            
        Returns:
            å“è©å
        """
        if token in self.particles:
            return 'åŠ©è©'
        elif token in self.technical_terms:
            return 'åè©-å›ºæœ‰åè©'
        elif self.kanji_pattern.fullmatch(token):
            return 'åè©'
        elif self.katakana_pattern.fullmatch(token):
            return 'åè©-å¤–æ¥èª'
        elif self.ascii_pattern.fullmatch(token):
            return 'åè©-è‹±èª'
        elif self.hiragana_pattern.fullmatch(token):
            return 'å‹•è©' if len(token) > 1 else 'åŠ©è©'
        else:
            return 'è¨˜å·'
    
    def _get_base_form(self, token: str) -> str:
        """
        åŸºæœ¬å½¢ã‚’å–å¾—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        
        Args:
            token: ãƒˆãƒ¼ã‚¯ãƒ³
            
        Returns:
            åŸºæœ¬å½¢
        """
        # ç°¡æ˜“çš„ãªæ´»ç”¨å‡¦ç†
        if token.endswith('ã—ã¾ã™'):
            return token[:-3] + 'ã™ã‚‹'
        elif token.endswith('ã—ãŸ'):
            return token[:-2] + 'ã™ã‚‹'
        elif token.endswith('ã§ã‚ã‚‹'):
            return 'ã '
        else:
            return token
    
    def _get_reading(self, token: str) -> str:
        """
        èª­ã¿ã‚’å–å¾—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        
        Args:
            token: ãƒˆãƒ¼ã‚¯ãƒ³
            
        Returns:
            èª­ã¿
        """
        # ã‚«ã‚¿ã‚«ãƒŠã¯ãã®ã¾ã¾ã€ãã®ä»–ã¯çœç•¥
        if self.katakana_pattern.fullmatch(token):
            return token
        else:
            return ''
    
    def _is_japanese(self, text: str) -> bool:
        """
        æ—¥æœ¬èªæ–‡å­—åˆ—ã‹ã©ã†ã‹åˆ¤å®š
        
        Args:
            text: åˆ¤å®šå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            æ—¥æœ¬èªæ–‡å­—åˆ—ã®å ´åˆTrue
        """
        return bool(
            self.hiragana_pattern.search(text) or
            self.katakana_pattern.search(text) or
            self.kanji_pattern.search(text)
        )
    
    def _is_noun_like(self, token: str) -> bool:
        """
        åè©çš„ãªãƒˆãƒ¼ã‚¯ãƒ³ã‹ã©ã†ã‹åˆ¤å®š
        
        Args:
            token: ãƒˆãƒ¼ã‚¯ãƒ³
            
        Returns:
            åè©çš„ãªå ´åˆTrue
        """
        pos = self._get_part_of_speech(token)
        return pos.startswith('åè©') and token not in self.particles


if __name__ == '__main__':
    # ä½¿ç”¨ä¾‹
    analyzer = JapaneseAnalyzer()
    
    test_texts = [
        "Slacké€šçŸ¥ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„",
        "ãƒ—ãƒªãƒšã‚¤ãƒ‰ã‚«ãƒ¼ãƒ‰ã®ãƒãƒ£ãƒ¼ã‚¸æ©Ÿèƒ½",
        "Dockerç’°å¢ƒã§ã®APIèªè¨¼ã‚·ã‚¹ãƒ†ãƒ ",
        "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®š"
    ]
    
    for text in test_texts:
        print(f"\nğŸ“ ãƒ†ã‚­ã‚¹ãƒˆ: {text}")
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokens = analyzer.tokenize(text)
        print(f"ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒ³: {tokens}")
        
        # å½¢æ…‹ç´ è§£æ
        morphemes = analyzer.analyze(text)
        print("ğŸ” å½¢æ…‹ç´ è§£æ:")
        for m in morphemes:
            print(f"  {m['surface']} ({m['pos']})")
        
        # è¤‡åˆèªæŠ½å‡º
        compounds = analyzer.extract_compound_words(text)
        if compounds:
            print(f"ğŸ”— è¤‡åˆèª: {compounds}")