#!/usr/bin/env python3
"""
ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸è‡ªå‹•ç”Ÿæˆå™¨ - Phase 2å®Ÿè£…
æ—¢å­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰å°‚é–€ç”¨èªã‚’è‡ªå‹•æŠ½å‡ºã—ã¦ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ã‚’æ§‹ç¯‰
"""

import json
import re
from typing import Dict, List, Set, Tuple
from pathlib import Path
import sqlite3
from collections import Counter, defaultdict

class DictionaryGenerator:
    """
    RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰å°‚é–€ç”¨èªã‚’æŠ½å‡ºã—ã¦ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ã‚’è‡ªå‹•ç”Ÿæˆ
    """
    
    def __init__(self, rag_db_path: str = None, output_path: str = None):
        """
        è¾æ›¸ç”Ÿæˆå™¨ã®åˆæœŸåŒ–
        
        Args:
            rag_db_path: RAGãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¹
            output_path: å‡ºåŠ›è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.rag_db_path = rag_db_path or "/home/ogura/.rag/vector_db.sqlite"
        self.output_path = output_path or "data/auto_generated_dictionary.json"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
        self.tech_patterns = {
            # APIãƒ»ãƒ—ãƒ­ãƒˆã‚³ãƒ«é–¢é€£
            'api_terms': re.compile(r'\b(?:API|REST|GraphQL|WebSocket|HTTP|HTTPS|SSL|TLS|OAuth|JWT|CORS)\b', re.IGNORECASE),
            
            # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªãƒ»ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
            'frameworks': re.compile(r'\b(?:Laravel|Vue\.js|React|Node\.js|Docker|Kubernetes|AWS|Python|JavaScript|PHP|Kotlin|Swift)\b', re.IGNORECASE),
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ»ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
            'database': re.compile(r'\b(?:PostgreSQL|MySQL|Redis|MongoDB|S3|RDS|DynamoDB|SQLite)\b', re.IGNORECASE),
            
            # ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»é‹ç”¨
            'infrastructure': re.compile(r'\b(?:CI/CD|DevOps|Terraform|Ansible|Jenkins|GitHub|GitLab|Bitbucket)\b', re.IGNORECASE),
            
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
            'security': re.compile(r'\b(?:èªè¨¼|ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£|æš—å·åŒ–|ãƒãƒƒã‚·ãƒ¥|HMAC|AES|RSA|è¨¼æ˜æ›¸|ãƒˆãƒ¼ã‚¯ãƒ³)\b'),
            
            # æ¥­å‹™ãƒ»æ±ºæ¸ˆé–¢é€£
            'business': re.compile(r'\b(?:ãƒ—ãƒªãƒšã‚¤ãƒ‰ã‚«ãƒ¼ãƒ‰|æ±ºæ¸ˆ|ãƒãƒ£ãƒ¼ã‚¸|æ®‹é«˜|VISA|QRã‚³ãƒ¼ãƒ‰|ATM|éŠ€è¡Œ|ã‚»ãƒ–ãƒ³éŠ€è¡Œ|ã‚¦ãƒ«ãƒˆãƒ©ãƒšã‚¤|PayBlend)\b'),
        }
        
        # æ—¥æœ¬èªè¤‡åˆèªãƒ‘ã‚¿ãƒ¼ãƒ³
        self.japanese_patterns = {
            'compound_katakana': re.compile(r'[ã‚¡-ãƒ¾]{2,}'),  # ã‚«ã‚¿ã‚«ãƒŠè¤‡åˆèª
            'compound_kanji': re.compile(r'[ä¸€-é¾¯]{2,}'),    # æ¼¢å­—è¤‡åˆèª
            'mixed_compound': re.compile(r'[a-zA-Z]+[ã²-ã‚ã‚¡-ãƒ¾ä¸€-é¾¯]+|[ã²-ã‚ã‚¡-ãƒ¾ä¸€-é¾¯]+[a-zA-Z]+'),  # æ··åœ¨è¤‡åˆèª
        }
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.exclude_patterns = [
            re.compile(r'^[0-9]+$'),  # ç´”ç²‹ãªæ•°å­—
            re.compile(r'^[a-z]{1,2}$'),  # çŸ­ã„è‹±å­—
            re.compile(r'^[ã²-ã‚]{1,2}$'),  # çŸ­ã„ã²ã‚‰ãŒãª
            re.compile(r'[^\w\s\-_]'),  # ç‰¹æ®Šè¨˜å·ã‚’å«ã‚€
        ]
    
    def generate_dictionary(self) -> Dict:
        """
        RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰è¾æ›¸ã‚’è‡ªå‹•ç”Ÿæˆ
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸè¾æ›¸ãƒ‡ãƒ¼ã‚¿
        """
        print("ğŸ” RAGãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å°‚é–€ç”¨èªã‚’æŠ½å‡ºä¸­...")
        
        # RAGãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ–‡æ›¸ã‚’å–å¾—
        documents = self._fetch_documents()
        print(f"ğŸ“„ {len(documents)} ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†å¯¾è±¡ã¨ã—ã¾ã™")
        
        # å°‚é–€ç”¨èªã‚’æŠ½å‡º
        extracted_terms = self._extract_technical_terms(documents)
        print(f"ğŸ”¤ {len(extracted_terms)} å€‹ã®å€™è£œç”¨èªã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
        
        # ç”¨èªã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        filtered_terms = self._filter_and_rank_terms(extracted_terms)
        print(f"âœ… {len(filtered_terms)} å€‹ã®ç”¨èªã‚’é¸å®šã—ã¾ã—ãŸ")
        
        # è¾æ›¸å½¢å¼ã«å¤‰æ›
        dictionary = self._build_dictionary(filtered_terms)
        print(f"ğŸ“š è¾æ›¸ã« {len(dictionary['compound_terms'])} å€‹ã®å°‚é–€ç”¨èªã‚’ç™»éŒ²ã—ã¾ã—ãŸ")
        
        # æ—¢å­˜è¾æ›¸ã¨ã®çµ±åˆ
        final_dictionary = self._merge_with_existing_dictionary(dictionary)
        
        # è¾æ›¸ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        self._save_dictionary(final_dictionary)
        
        return final_dictionary
    
    def _fetch_documents(self) -> List[Dict[str, str]]:
        """
        RAGãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        
        Returns:
            ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        documents = []
        
        try:
            if not Path(self.rag_db_path).exists():
                print(f"âš ï¸  RAGãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.rag_db_path}")
                return []
            
            conn = sqlite3.connect(self.rag_db_path)
            cursor = conn.cursor()
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚’ç¢ºèª
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = cursor.fetchall()
            print(f"ğŸ“Š åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ¼ãƒ–ãƒ«: {[t[0] for t in tables]}")
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å–å¾—ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«åã¯ç’°å¢ƒã«ä¾å­˜ï¼‰
            possible_queries = [
                "SELECT text, metadata FROM documents LIMIT 1000",
                "SELECT content, title FROM documents LIMIT 1000", 
                "SELECT text FROM chunks LIMIT 1000",
                "SELECT content FROM content LIMIT 1000"
            ]
            
            for query in possible_queries:
                try:
                    cursor.execute(query)
                    rows = cursor.fetchall()
                    
                    for row in rows:
                        if len(row) >= 2:
                            documents.append({
                                'text': row[0] or '',
                                'metadata': row[1] or ''
                            })
                        else:
                            documents.append({
                                'text': row[0] or '',
                                'metadata': ''
                            })
                    
                    if documents:
                        print(f"âœ… ã‚¯ã‚¨ãƒªæˆåŠŸ: {query}")
                        break
                        
                except sqlite3.Error:
                    continue
            
            conn.close()
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥èª­ã¿å–ã‚Š
        if not documents:
            print("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç”¨èªæŠ½å‡º")
            documents = self._load_project_files()
        
        return documents
    
    def _load_project_files(self) -> List[Dict[str, str]]:
        """
        ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥èª­ã¿å–ã‚Š
        
        Returns:
            ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®ãƒªã‚¹ãƒˆ
        """
        documents = []
        project_root = Path("/home/ogura/work/ultra")
        
        # ä¸»è¦ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
        doc_patterns = ["**/*.md", "**/*.txt", "**/*.json", "**/*README*"]
        
        for pattern in doc_patterns:
            try:
                for file_path in project_root.glob(pattern):
                    if file_path.is_file() and file_path.stat().st_size < 1024 * 1024:  # 1MBæœªæº€
                        try:
                            content = file_path.read_text(encoding='utf-8', errors='ignore')
                            documents.append({
                                'text': content,
                                'metadata': str(file_path)
                            })
                        except Exception:
                            continue
            except Exception:
                continue
                
        return documents[:100]  # æœ€å¤§100ãƒ•ã‚¡ã‚¤ãƒ«
    
    def _extract_technical_terms(self, documents: List[Dict[str, str]]) -> Counter:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ€è¡“ç”¨èªã‚’æŠ½å‡º
        
        Args:
            documents: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
            
        Returns:
            ç”¨èªã®å‡ºç¾é »åº¦
        """
        term_counter = Counter()
        
        for doc in documents:
            text = doc['text'] + ' ' + doc['metadata']
            
            # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ç”¨èªã‚’æŠ½å‡º
            for category, pattern in self.tech_patterns.items():
                matches = pattern.findall(text)
                for match in matches:
                    term_counter[match.strip()] += 1
            
            # æ—¥æœ¬èªè¤‡åˆèªã®æŠ½å‡º
            for category, pattern in self.japanese_patterns.items():
                matches = pattern.findall(text)
                for match in matches:
                    if len(match) >= 2 and not any(exc.search(match) for exc in self.exclude_patterns):
                        term_counter[match.strip()] += 1
        
        return term_counter
    
    def _filter_and_rank_terms(self, term_counter: Counter) -> List[Tuple[str, int]]:
        """
        ç”¨èªã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦ãƒ©ãƒ³ã‚¯ä»˜ã‘
        
        Args:
            term_counter: ç”¨èªã®å‡ºç¾é »åº¦
            
        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ç”¨èªãƒªã‚¹ãƒˆ
        """
        filtered_terms = []
        
        for term, count in term_counter.most_common():
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶
            if (count >= 2 and  # æœ€ä½2å›ã¯å‡ºç¾
                len(term) >= 2 and  # æœ€ä½2æ–‡å­—
                len(term) <= 20 and  # æœ€å¤§20æ–‡å­—
                not any(exc.search(term) for exc in self.exclude_patterns)):
                
                filtered_terms.append((term, count))
        
        return filtered_terms[:500]  # ä¸Šä½500èªã¾ã§
    
    def _build_dictionary(self, terms: List[Tuple[str, int]]) -> Dict:
        """
        ç”¨èªãƒªã‚¹ãƒˆã‹ã‚‰è¾æ›¸ã‚’æ§‹ç¯‰
        
        Args:
            terms: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ç”¨èªãƒªã‚¹ãƒˆ
            
        Returns:
            è¾æ›¸ãƒ‡ãƒ¼ã‚¿
        """
        compound_terms = {}
        
        for term, frequency in terms:
            # é‡ã¿ç®—å‡ºï¼ˆé »åº¦ãƒ™ãƒ¼ã‚¹ã€æœ€å¤§1.0ï¼‰
            weight = min(1.0, frequency / 10.0)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²ã®æ¨å®š
            tokens = self._estimate_tokens(term)
            
            # åŒç¾©èªã®ç”Ÿæˆ
            synonyms = self._generate_synonyms(term)
            
            compound_terms[term] = {
                'tokens': tokens,
                'synonyms': synonyms,
                'weight': weight,
                'frequency': frequency,
                'category': self._categorize_term(term)
            }
        
        return {
            'compound_terms': compound_terms,
            'meta': {
                'generated_at': '2025-01-24',
                'version': '2.0.0',
                'source': 'auto_generated',
                'total_terms': len(compound_terms)
            }
        }
    
    def _estimate_tokens(self, term: str) -> List[str]:
        """
        ç”¨èªã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²ã‚’æ¨å®š
        
        Args:
            term: å¯¾è±¡ç”¨èª
            
        Returns:
            æ¨å®šã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆ
        """
        # ç°¡æ˜“çš„ãªãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²ãƒ­ã‚¸ãƒƒã‚¯
        tokens = []
        
        # è‹±èª+æ—¥æœ¬èªã®å¢ƒç•Œã§åˆ†å‰²
        parts = re.split(r'(?<=[a-zA-Z])(?=[ã²-ã‚ã‚¡-ãƒ¾ä¸€-é¾¯])|(?<=[ã²-ã‚ã‚¡-ãƒ¾ä¸€-é¾¯])(?=[a-zA-Z])', term)
        
        for part in parts:
            if part:
                tokens.append(part.strip())
        
        return tokens if len(tokens) > 1 else [term]
    
    def _generate_synonyms(self, term: str) -> List[str]:
        """
        åŒç¾©èªã‚’ç”Ÿæˆ
        
        Args:
            term: å¯¾è±¡ç”¨èª
            
        Returns:
            åŒç¾©èªã®ãƒªã‚¹ãƒˆ
        """
        synonyms = []
        
        # ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šç‰ˆ
        if re.search(r'[a-zA-Z][ã²-ã‚ã‚¡-ãƒ¾ä¸€-é¾¯]', term):
            spaced = re.sub(r'([a-zA-Z]+)([ã²-ã‚ã‚¡-ãƒ¾ä¸€-é¾¯]+)', r'\\1 \\2', term)
            synonyms.append(spaced)
        
        # ã‚«ã‚¿ã‚«ãƒŠ/ã²ã‚‰ãŒãªå¤‰æ›ï¼ˆç°¡æ˜“ï¼‰
        if re.search(r'[ã‚¡-ãƒ¾]', term):
            hiragana_variant = self._katakana_to_hiragana(term)
            if hiragana_variant != term:
                synonyms.append(hiragana_variant)
        
        # è‹±èªç¿»è¨³ï¼ˆæ—¢çŸ¥ã®ã‚‚ã®ã®ã¿ï¼‰
        translations = {
            'é€šçŸ¥': 'notification',
            'è¨­å®š': 'settings', 
            'èªè¨¼': 'authentication',
            'ç’°å¢ƒ': 'environment',
            'å¤‰æ•°': 'variable',
            'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹': 'database',
            'ã‚µãƒ¼ãƒãƒ¼': 'server'
        }
        
        for jp, en in translations.items():
            if jp in term:
                synonyms.append(term.replace(jp, en))
        
        return list(set(synonyms))  # é‡è¤‡é™¤å»
    
    def _categorize_term(self, term: str) -> str:
        """
        ç”¨èªã‚’ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
        
        Args:
            term: å¯¾è±¡ç”¨èª
            
        Returns:
            ã‚«ãƒ†ã‚´ãƒªå
        """
        for category, pattern in self.tech_patterns.items():
            if pattern.search(term):
                return category
        
        if re.search(r'[ã‚¡-ãƒ¾]', term):
            return 'katakana'
        elif re.search(r'[ä¸€-é¾¯]', term):
            return 'kanji'
        elif re.search(r'[a-zA-Z]', term):
            return 'english'
        else:
            return 'other'
    
    def _katakana_to_hiragana(self, text: str) -> str:
        """
        ã‚«ã‚¿ã‚«ãƒŠã‚’ã²ã‚‰ãŒãªã«å¤‰æ›ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        
        Args:
            text: å¤‰æ›å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            ã²ã‚‰ãŒãªå¤‰æ›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        result = ""
        for char in text:
            if 'ã‚¡' <= char <= 'ãƒ¾':
                # ã‚«ã‚¿ã‚«ãƒŠã‚’ã²ã‚‰ãŒãªã«å¤‰æ›
                hiragana_char = chr(ord(char) - ord('ã‚¡') + ord('ã'))
                result += hiragana_char
            else:
                result += char
        return result
    
    def _merge_with_existing_dictionary(self, new_dict: Dict) -> Dict:
        """
        æ—¢å­˜è¾æ›¸ã¨ãƒãƒ¼ã‚¸
        
        Args:
            new_dict: æ–°ã—ã„è¾æ›¸ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            ãƒãƒ¼ã‚¸ã•ã‚ŒãŸè¾æ›¸
        """
        existing_path = "data/compound_terms.json"
        
        if not Path(existing_path).exists():
            return new_dict
        
        try:
            with open(existing_path, 'r', encoding='utf-8') as f:
                existing_dict = json.load(f)
            
            # æ—¢å­˜ã®ç”¨èªã‚’å„ªå…ˆã—ã¦ãƒãƒ¼ã‚¸
            merged_terms = existing_dict.get('compound_terms', {}).copy()
            
            for term, data in new_dict['compound_terms'].items():
                if term not in merged_terms:
                    merged_terms[term] = data
                else:
                    # æ—¢å­˜ç”¨èªã®å ´åˆã¯é »åº¦æƒ…å ±ã®ã¿æ›´æ–°
                    merged_terms[term]['auto_frequency'] = data.get('frequency', 0)
            
            return {
                'compound_terms': merged_terms,
                'meta': {
                    'updated_at': '2025-01-24',
                    'version': '2.0.0',
                    'total_terms': len(merged_terms),
                    'auto_generated_count': len(new_dict['compound_terms'])
                }
            }
            
        except Exception as e:
            print(f"âš ï¸  æ—¢å­˜è¾æ›¸ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return new_dict
    
    def _save_dictionary(self, dictionary: Dict):
        """
        è¾æ›¸ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        
        Args:
            dictionary: ä¿å­˜ã™ã‚‹è¾æ›¸ãƒ‡ãƒ¼ã‚¿
        """
        output_path = Path(self.output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(dictionary, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")


if __name__ == '__main__':
    # ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ã®è‡ªå‹•ç”Ÿæˆ
    generator = DictionaryGenerator()
    
    print("ğŸš€ ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ã®è‡ªå‹•ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™...")
    dictionary = generator.generate_dictionary()
    
    print("\nğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸè¾æ›¸ã®ã‚µãƒãƒªãƒ¼:")
    print(f"  - ç·ç”¨èªæ•°: {len(dictionary['compound_terms'])}")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
    categories = {}
    for term, data in dictionary['compound_terms'].items():
        cat = data.get('category', 'unknown')
        categories[cat] = categories.get(cat, 0) + 1
    
    print("  - ã‚«ãƒ†ã‚´ãƒªåˆ¥ç”¨èªæ•°:")
    for cat, count in sorted(categories.items()):
        print(f"    {cat}: {count}èª")
    
    print("\nâœ… ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ç”Ÿæˆå®Œäº†!")